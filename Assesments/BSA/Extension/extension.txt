My primary aim with this piece of work is to reduce the computaitonal complexity, and therefore the runtime,
without utilising a vast amount of unneccessary memory. Let's start of by looking at the big O complexities 
of each of the functions used in the main secion of this CW. 

Current complexities of my implementation of BSA methods:

bsa_init(): O(log(n))
--
bsa_set(): Mostly O(log2(n)), unless new row is created, then O(n)
    -> pointer_index() O(log(n))
    -> bsa_row_create() O(n)
    -> col_index() O(1)
        -> pointer_index() O(log(n))
    -> bsa_append() O(1)
--
bsa_get(): Always O(log2(n))
    -> pointer_index() O(log(n))
    -> col_index() O(1)
        -> pointer_index() O(log(n))
    -> used_cell() O(1)
--
bsa_delete(): Mostly O(log2(n)), unless largest index is removed, then O(n)
    -> pointer_index() O(log(n))
    -> col_index() O(1)
        -> pointer_index() O(log(n))
    -> used_cell() O(1)
    -> bsa_remove() O(1)
        -> bsa_row_free() O(1)
    -> next_maxindex() O(n)
--
bsa_maxindex(): O(1)
--
bsa_free(): O(log(n))
--
bsa_foreach():  O(n) - Always O(n) as it loops through all elements
    -> bsa_get() O(1)
        -> pointer_index() O(log(n))
        -> col_index() O(1) 
            -> pointer_index() O(log(n))
        -> used_cell() O(1)

Overall, it is a somewhat efficient method. I feel there is alot of time wasted with the identification of row and column
indicies. As the main method of retrieving values, this function is called far more than any other, this should be optimised as 
a priority. Other functions that should be addressed is the allocation of memory, where the row_creation function has a 
complexity of O(n), and table initialisation has a complexity of O(log(n)). 

Since the type of storage at hand is series of mostly unique integers, a hash table is a very appropriate method of storage,
futhermore, hashing lends itself very nicely to reducing computational complexity. However, normal hasing where one hashes by
value wouldn't be appropriate here as the test functions (sieve, isfactorial & fibmemo) all use index geometry to calculate new
values. Therefore we need to hash by insertion index so when we need to know the 3rd number in the fibmemo series for example,
we can access it by hashing index 2. This will also allow bsa_get() to work more effectively, accessing the correct values. 

The chosen hash function was a simple one, for a value sent ot index 'i', the index was given by:
hash_index = i % table_length.

Collisions were dealt with using a neighbourhood system, i.e, when a collision occured, the incoming value was inserted in the closest 
cell up to +-3 cells away (this value can be adjusted though). If no cells are avaialble within this area, a reallocation is triggered, 
where the new list size is 1 higher

This reallocation and rehashing should resolve all conflicts. This process is repeated until all current values and the new incoming value can 
fit into the array. 

While this can potentially lead to overly-frequent reallocations and rehashing, the small nature of the data set makes this eventuality 
unlikely. There is an additional reallocation claused added into the bsa_set() function, where if the array load factor (portion filled)
exceeds 0.75, the array is reallocated to a size 1.5 times bigger, so that the load factor goes down to 0.5. The upper limit of 0.75 was chosen
as online reasearch showed that beyond this point, collision avoidance starts to significantly effect the efficiency of the algorithm. 

Let's look at the complexities of the hashing technique:

bsa_init(): O(1)
--
bsa_set(): Mostly O(1), unless a reallocation is triggered, then O(n)
    -> neighbour_availability() O(1)
    -> bsa_resize() O(n)
        -> bsa_reallocate() O(n)
    -> hash_function() O(1)
--
bsa_get(): O(1)
    -> hash_function() O(1)
--
bsa_delete() - Mostly O(1), unless largest index is removed, then O(n) 
    -> hash_function() O(1)
    -> bsa_get() O(1)
    -> next_maxindex() O(n)

--
bsa_maxindex() O(1)
--
bsa_free() O(1)
--
bsa_foreach() O(n) - Always O(n) as it loops through all elements
    -> bsa_get() O(1)

Comparing the 2 tables, the hashing method is far less computationally complex. Let's see if this is reflected at runtime and if the 
reallocation significantly detriments performance. To strike a balance between saving memory while preventing unneccessary reallocations
a moderate initial table size of 50 was chosen. 

I decided to run a profiler to comare the algorithms for each of the 3 scripts. The time results were collected:

Script        |    Time  |  
sieve         |   0.004  |       
isfactorial   |   12.26  |
fibmemo       |   0.004  |

e_sieve       |   0.003  |
e_isfactorial |   2.301  |
e_fibmemo     |   0.003  |

It was hard to differentiate between the 2 methods for sieve and fibmemo scripts as they were relatively simple to begin with.
However for the isfactorial script, there was a clear improvment. This can be explained by their respective profiles. In the
original BSA method, the two main functions called, accounting for 57% of function calls, were pointer_index() (O(log(n))) and
bsa_row_create() (O(n)), whereas in the hash array, the primary function called, accounting for 84% of function calls was
bsa_get (O(1)). This explains the significantly lower runtime. and shows that the reallocation did not effect overall performance.



